\renewcommand{\O}{\mathcal{O}}

% a brief report explaining the design and implementation of your algorithms, as well
% as a set of comprehensive experimental results, clearly analyzed. The design and
% implementation explanations should not exceed 2 pages.

\section{Introduction}
% 1. As an introduction, include the problem requirements and briefly explain your algorithm.

For the first assignment, assignment A,  it was required to implement an efficient parallel program in PThreads and OpenMP to compute the prefix and suffix minima of a given array. Moreover a sequential implementation from which the remaining are inferred should also be given.

The suffix minima problem is defined as determining the $min\{a_i, a_{i+1},\dots,a_N\}$ for each $i$. Where the array $A = (a_1, \dots, a_N)$. On the other hand the prefix minima is the $min{a_1, a_2, \dots, a_i}$ for each $i$.

The algorithm to compute the prefix and suffix minima should have a time complexity $\O(\log_2(N))$. 
The solution that was implemented will be first introduced in this section and further explained in section~\ref{sec:proof}.
Sequentially these problems are very simple to solve with one loop that checks what is the current minimum value of the array. However, in order to parallelize this algorithm and obtain the desired time complexity we will need organize the tasks in a binary tree with height $\log_2(N)$.

\section{Algorithm Proof}
\label{sec:proof}
% 2. Give an algorithm proof: show why the algorithm will give the desired results.
% For example, if the exercise is: calculate n!, and your algorithm is: fac(0)=1 and fac(n)=n · fac(n − 1) you should include a proof by induction.


Starting with sequential prefix minima. Initially an auxiliary variable, $m$, minimum is set as the first value of the array, $P[0]$. Thereafter one simply iterates over the complete $P$ and, at each iteration, $i$ checks, if the current value is lower than the minimum. If that is the case the minimum is updated to the current value of the array. Finally the array values is set as the current minimum, \emph{i.e.} $P[i] = m$.
The sequential version of the suffix minima is similar to the previous but the initial $i$ is $N-1$ (instead of $0$), where $N$ is the length of the array. Thereafter the index is decremented until 0.
This algorithm has a time complexity of $\O(N)$, since it only has one loop with $N$ iterations.


The parallelized algorithm should have a time complexity of $\O(\log_2(N))$. In order to obtain it\footnote{This algorithm is based on a presentation by Ahmad Khayyat from Queen's University, that can be found at \url{http://www.slideshare.net/akhayyat/parallel-algorithms-for-trees}} we will divide the tasks in a binary tree form, where tasks in the same level can be independently and concurrently executed. Therefore the height (and time complexity) is $\O(\log_2(N))$, as desired.
The pseudocode for this algorithm is show in Algorithm~\ref{alg:minima}. The preffixMinima and suffixMinima functions shown simply compare the middle values based on the min\_i and max\_i values and decide whether to concatenate or update the appropriate subarrays coming from their children.

\begin{algorithm}[H]
\caption{Calculate Prefix and Suffix Minima for $A$}
\label{alg:minima}
\begin{algorithmic}[1]
	\STATE $\Delta \leftarrow 1$
	\FOR{ $i \leftarrow 0$ \TO $\log_2(N)$}
		\STATE $\Delta \leftarrow \Delta \cdot 2$
		\renewcommand{\algorithmicdo}{\textbf{pardo}}
		\FOR{ $j \leftarrow 0$ \TO $N-1$}
			\STATE prefixMinima($j$,$j+r-1$); // prefixMinima(min\_i, max\_i)
			\STATE suffixMinima($j$,$j+r-1$); // suffixMinima(min\_i, max\_i)
		\ENDFOR
	\ENDFOR
\end{algorithmic}
\end{algorithm}


At the bottom of the tree each thread will receive two values and compute the prefix and suffix minima of them. In the next level each thread will receive twice as many inputs, in this case 4. However since they are already sorted it is only necessary to look at the end and beginning value for the arrays coming from the left and right leaf, respectively. By inspecting these two it is possible to understand if the arrays can be concatenated or need to be updated. After finishing this process we have the suffix and prefix minima for those four values. Continuing this process, $\log_2(N)$ times, \emph{i.e.} until $N$ inputs are fed to a single thread we obtain the complete solution.

\section{Time-Complexity Analysis}
% 3. Give an analysis of the time-complexity of your algorithm related to the size of the problem (usually N ).

Observing algorithm~\ref{alg:minima} it is easy to see the time complexity of this algorithm is $\O(\log_2(N))$. The inner loop on line 4 is  $\O(1)$ since all tasks are done concurrently. On the other hand the outer loop requires $\O(\log_2(N))$. Therefore the complete algorithm needs as much time as intended.

\section{Implementation}
% 4. Implement first a sequential algorithm. Next, add not more than 3 OpenMP pragmas to parallelize the time consuming sections/loops of the code. Further,
% implement the algorithm using PThreads. Explain the transition from pseudocode (or algorithm) to the implementation, focusing especially on work and data distribution.

This section will address the transition from the pseudocode to the Pthreads and OpenMP implementations. Starting with the latter, in essence only a pair of pragmas were necessary to parallelize the loops. In addition a chunk size was set based on the input size and the number of threads available.

Pthreads requires more work but also more control over the resulting program. Similarly to OpenMP each thread will compute the prefix and suffix minimas based on the outputs from their children. However the threads are spawn by the main thread and later joined. The join occurs when all tasks for that level are finished or when there are more tasks than threads. No barriers were used in the implementation.

\section{Results}
\label{sec:results}
% 5. Test your program varying both the problem size (at least 7 different problem
% sizes) and the number of threads (at least 1, 2, 4, 8, and 16 threads - P ). Measure
% the execution time, T , for each of the test cases (be sure to run the application
% multiple times - i.e. 10, 100, or even 1000 times, and average the results).

% Present your results in the following graphs: T (N, P fixed), T (P, N fixed), Speed-up(N, P
% Fixed), Speed-up(P, N fixed). Analyze the results, focusing on the difference (if
% any) between the measured and expected performance. See some more guidelines
% and requirements in subsection 1.5.

The program was tested with the example inputs from the manual and the results are summarized in table~\ref{tbl:results}. The machine was the one provided by the course and was accessed remotely via ssh.

\begin{table}[H]
\centering
\begin{tabular}{lcccccccc}
         & NSize & Iterations & Seq      & Th01      & Th02      & Th04      & Th08      & Th16
\\\midrule
Pthreads & 32    &     1000   & 0.000955 &  1.056421 &  0.578142 &  0.583764 &  2.072299 &  1.680798\\
OpenMP   & 32    &     1000   & 0.001256 &  0.010129 &  0.037573 &  0.087985 &  0.164634 &  0.276545
\end{tabular}
\caption{Timing results for the Parallel Implementations}
\label{tbl:results}
\end{table}

From table~\ref{tbl:results} one can see that for the input size used a threaded implementation is not advantageous. The work each task is responsible for is too small to compensate for the communication overhead. In addition the Pthreads implementation is significantly slower that OpenMP. Therefore it is likely not implemented in the most efficient way and should be revised for better performance. Nonetheless, the best OpenMP result is still around 10 times slower that the sequential algorithm.

\section{Conclusions}
% 6. Include a Conclusions section to briefly explain what have you done and, more
% important, what have you learned from the given exercise and its parallelization.
% Comment on efficiency and usability of the given implementation language/library
% for the given application and, if possible, generalize.

In this assignment it was required to implement a threaded algorithm to compute the prefix and suffix minima using Pthreads and OpenMP.
The algorithm chosen has a time complexity of $\O(\log_2(N))$ and it is based on a binary tree.
From section~\ref{sec:results} one can infer that for the input size used the communication overhead overwhelms the gains from concurrent execution.

On a personal note, this was the first time I programmed using threads. Therefore it was challenging, in particular at the beginning with Pthreads, to convert the pseudocode to a real, working code. Likely for that reason the results from Pthreads are not as satisfactory as OpenMP's.